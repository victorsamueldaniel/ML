{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning: Exercise session 06"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise session we will focus on Logistic regression and decision trees.\n",
    "\n",
    "In the first problem, we will consider the [ionosphere dataset](https://www.openml.org/d/59) and fit different logistic regression models.\n",
    "The goal is to predict whether a given radar signal from the ionosphere is \"good\" or \"bad\" based on 34 predictors.\n",
    "\n",
    "In the second problem, we will focus on decision trees.\n",
    "\n",
    "In the third problem, we will derive the log-likelihood function for logistic regression.\n",
    "\n",
    "**Note:** the third problem is theoretical and __not__ needed for the hand in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Import the dataset `ionosphere.csv`, separate the predictors and the target variable and split it into training and test set. When you perform the splitting, set `random_state=12` and `test_size=151`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Define a `Pipeline` where you apply the `StandardScaler` and then fit a `LogisticRegression` with the following tuning parameters: `penalty=\"none\"`, `solver=\"saga\"`, `tol=0.1`, `random_state=10`. Call the pipeline `pipe_logistic`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Fit the pipeline to the training data and predict the accuracy on the training and test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now want to fit a logistic regression with the lasso penalty. To do that we need to choose the best regulatization parameter. \n",
    "\n",
    "* Define a `Kfold` object with 10 splits, and `random_state=919`. Remeber to shuffle the rows. Name the object `folds`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Define a `Pipeline` where you first apply the `StandardScaler` (name this step `\"scaler\"`) and then fit a `LogisticRegression` (name this step `\"logistic\"`) with the following tuning parameters: `C=1`, `penalty=\"l1\"`, `solver=\"saga\"`, `tol=0.1`, `random_state=10`. Name the pipeline `pipe_logistic_l1`. What is the role of `C`? How does it compare to the tuning parameter `alpha` from `Lasso`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Define now a `GridSearchCV` object to perform the cross-validation over a grid of different values for `C`. Fill in the `??`.\n",
    "\n",
    "_Hint_: Choose carefully your grid, by trying out different possibility and looking at the cross-validated accuracy plot (see below). Ideally, you want a grid around the values of `C` that are \"interesting\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define CV object\n",
    "grid = 10 ** np.linspace(??, ??, 100)\n",
    "logistic_l1_cv = GridSearchCV(\n",
    "    estimator=pipe_logistic_l1,\n",
    "    param_grid={\"logistic__C\": grid},\n",
    "    scoring=\"accuracy\",\n",
    "    cv=folds,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Perform the cross-validation on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Plot the results of the cross-validation by filling in the `??`.\n",
    "\n",
    "_Note_: Notice that we want to compute the best model according to the __1-se rule__. To do that, we need to compute the standard error of the __mean__ cross-validated accuracy. This is obtained by dividing the standard deviations by the square root of $K = 10$ (i.e., number of folds)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results of cross-validation\n",
    "cv_res = logistic_l1_cv\n",
    "\n",
    "mean_scores = cv_res.cv_results_[??]\n",
    "se_scores = cv_res.cv_results_[??] / np.sqrt(10) # ! very important to divide by sqrt(# folds)\n",
    "params = cv_res.cv_results_[??].data\n",
    "\n",
    "best_index = np.argmax(mean_scores)\n",
    "\n",
    "one_se_best_param = np.min(\n",
    "    grid[mean_scores ?? mean_scores[best_index] ?? se_scores[best_index]]\n",
    ")\n",
    "\n",
    "one_se_score = mean_scores[grid == one_se_best_param][0]\n",
    "\n",
    "# just for reference, we also plot the parameter that maximizes the cross-validated accuracy\n",
    "best_param = cv_res.best_params_[\"logistic__C\"] \n",
    "best_score = cv_res.best_score_\n",
    "\n",
    "plt.errorbar(x=np.log10(grid), y=mean_scores, yerr=se_scores, fmt=\"o\", capsize=3)\n",
    "\n",
    "plt.axvline(\n",
    "    np.log10(best_param), ls=\"dotted\", color=\"grey\"\n",
    ")  # vertical line at the parameter value yielding highest accuracty\n",
    "plt.axhline(\n",
    "    best_score, ls=\"dotted\", color=\"grey\"\n",
    ")  # horizontal line at highest accuracy\n",
    "plt.axvline(\n",
    "    np.log10(one_se_best_param), ls=\"dotted\", color=\"green\"\n",
    ")  # vertical line at the parameter corresponding to 1-se\n",
    "plt.axhline(\n",
    "    one_se_score, ls=\"dotted\", color=\"green\"\n",
    ")  # horizontal line at corresponding to 1-se parameter\n",
    "\n",
    "plt.title(\"Logistic-l1 CV Accuracy\")\n",
    "plt.xlabel(\"log(C)\")\n",
    "plt.ylabel(\"CV Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Finally, refit the __1-se rule__ model to the whole training dataset, and compute the accuracy on the training and test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Does this model perform better than the unregularized logistic regression? What are the advantages of this model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-17T12:11:05.480327Z",
     "start_time": "2021-04-17T12:11:03.434226Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold, train_test_split, GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.0 Data import and Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the \"Heart\" dataset, containing demographic and medical informations about a list of patients. Drop `NA` values, convert the categorical variables into the desired pandas type: \"category\", and perform any other necessary data cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-17T12:11:05.549173Z",
     "start_time": "2021-04-17T12:11:05.480327Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-17T12:11:05.580385Z",
     "start_time": "2021-04-17T12:11:05.549173Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The response variable that we will try to predict is \"AHD\", that indicates whether the patient has a heart disease.\n",
    "\n",
    "Now split the data into the features `X` and the response `y`.\n",
    "\n",
    "As, sadly, trees in sklearn do not handle categorical features directly, transform `X`'s categorical variables into \"dummies\". This can be achieved either with panda's `pd.get_dummies(<data>, drop_first=True)` or sklearn's `OneHotEncoder`. Make sure that you keep X in dataframe format, and keep feature/column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-17T12:11:05.618177Z",
     "start_time": "2021-04-17T12:11:05.580385Z"
    }
   },
   "outputs": [],
   "source": [
    "X = \n",
    "y = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into train and test. Set random_state=40 and leave 20% of the data out for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-17T12:11:05.633799Z",
     "start_time": "2021-04-17T12:11:05.618177Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Unpruned tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-17T12:11:05.780888Z",
     "start_time": "2021-04-17T12:11:05.633799Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier, plot_tree, export_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Begin by fitting a (unpruned) classification tree to the training data, in order to predict whether each patient potentially has AHD, based on his available demographic and medical information. Use either the Gini index or the entropy as a splitting criterion, as you prefer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-17T12:11:05.796508Z",
     "start_time": "2021-04-17T12:11:05.780888Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now plot the tree using sklearn's `plot_tree` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-17T12:11:08.355333Z",
     "start_time": "2021-04-17T12:11:05.796508Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,14))\n",
    "??\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Could you use only this image to predict the \"AHD\" variable for the observations left out in the test set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now check the model's training and test accuracy (or score). What do you observe and why ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-17T12:11:08.370955Z",
     "start_time": "2021-04-17T12:11:08.355333Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Train accuracy:\", ??)\n",
    "print(\"Test accuracy:\", ??)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Cost-complexity pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that the flexibility of a decision tree can be tuned by pruning it via cost-complexity pruning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use cross-validation to select the best cost-complexity hyper-parameter $\\alpha$ value. Then, plot the tree coresponding to the best $\\alpha$ value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-17T12:11:08.386578Z",
     "start_time": "2021-04-17T12:11:08.370955Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-17T12:11:08.408711Z",
     "start_time": "2021-04-17T12:11:08.386578Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-17T12:11:09.625937Z",
     "start_time": "2021-04-17T12:11:08.408711Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-17T12:11:09.653267Z",
     "start_time": "2021-04-17T12:11:09.625937Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-17T12:11:09.841838Z",
     "start_time": "2021-04-17T12:11:09.653267Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-17T12:11:09.857395Z",
     "start_time": "2021-04-17T12:11:09.841838Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-17T12:11:10.259079Z",
     "start_time": "2021-04-17T12:11:09.857395Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,9))\n",
    "??\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3 (not to hand in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have independent observations $(x_1,y_1),\\dots, (x_n,y_n)$ from $(X,Y)$ with $x_i \\in \\mathbb R^p$, and $y_i \\in \\{0,1\\}$,  $i = 1, \\dots, n$.\n",
    "Further assume that the conditional distribution of $Y$ given $X= x$ is Bernoulli with success probability denoted as:\n",
    "  $$ \\mathbb P(Y = 1 \\mid X = x) = 1 - \\mathbb P(Y = 0 \\mid X = x) = \\sigma(x^\\top \\beta),$$\n",
    "where $\\sigma: \\mathbb R \\to [0, 1]$, and\n",
    "  $$\n",
    "  \\sigma(a) = \\dfrac{e^{a}}{1 + e^{a}}, \\quad a\\in \\mathbb R.\n",
    "$$\n",
    "Show that the log-likelihood of the parameter vector $\\beta\\in\\mathbb R^p$ is\n",
    "  $$\\ell(\\beta) = \\sum_{i=1}^n y_i x_i^\\top \\beta - \\log(1 + e^{x_i^\\top \\beta}).$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "216px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
